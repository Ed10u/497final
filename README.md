# Confidenceâ€‘Aware Retrievalâ€‘Augmented Generation (CAâ€‘RAG)

> **A minimal, fullyâ€‘reproducible reference pipeline** that combines confidenceâ€‘weighted retrieval with LoRAâ€‘fineâ€‘tuned Llamaâ€‘3Â 8B to reduce hallucinations in scientific questionâ€‘answering.

---

## âœ¨ Key Features

| Stage | What it does | Where to look |
|-------|--------------|---------------|
| **0Â Â·Â Config & logging** | Centralised hyperâ€‘parameters, reproducible seeding | `Config` dataclass in `ca_rag_pipeline.py` |
| **1Â Â·Â Dataset builder** | Pulls 200 PubMed abstracts, slices them into *problem / approach* pairs, builds an 80/10/10 split | `fetch_pubmed_abstracts()` & `build_dataset()` |
| **2Â Â·Â Confidenceâ€‘aware retrieval** | BM25 overlapÂ + journal credibilityÂ + recency â†’ single score | `confidence_score()` & `Retriever` class |
| **3Â Â·Â Model fineâ€‘tuning** | LoRA adapters on **Metaâ€‘Llamaâ€‘3â€‘8Bâ€‘Instruct** with hyperâ€‘params from TableÂ 2 | `finetune()` |
| **4Â Â·Â Evaluation** | ROUGEâ€‘L + BERTScore automatic metrics (mirrors the report) | `evaluate()` |

---

## ðŸ› Â Requirements

```bash
pythonÂ >=Â 3.10
CUDAâ€‘enabled GPUÂ (â‰ˆ12Â GBÂ VRAM for model + LoRA fineâ€‘tuning)
```

Install Python dependencies:

```bash
pip install transformers accelerate peft datasets rank-bm25 \
            scikit-learn rouge-score bert-score biopython tqdm
```

---

## ðŸš€Â QuickÂ Start

```bash
# 1Â â–¸ Clone this repo & cd in
python ca_rag_pipeline.py            # oneâ€‘shot: build âžœ train âžœ evaluate

# 2Â â–¸ Need help?
python ca_rag_pipeline.py --help      # list CLI flags
```

Artifacts are written to `./artifacts/` (dataset cache, LoRA checkpoints, metrics logs).

---

## ðŸ“‚Â Project Layout

```
.
â”œâ”€â”€ ca_rag_pipeline.py   # singleâ€‘file pipeline (youâ€™re here ðŸ“œ)
â”œâ”€â”€ README.md            # project overview (this file)
```

---


